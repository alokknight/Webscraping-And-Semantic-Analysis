{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sre_yield\n",
    "import pandas as pd\n",
    "import requests\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Alok Patel\n",
      "[nltk_data]     erma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Alok Patel\n",
      "[nltk_data]     erma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "      <th>Unnamed: 24</th>\n",
       "      <th>Unnamed: 25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  Unnamed: 2  \\\n",
       "0       1  https://insights.blackcoffer.com/how-is-login-...         NaN   \n",
       "1       2  https://insights.blackcoffer.com/how-does-ai-h...         NaN   \n",
       "2       3  https://insights.blackcoffer.com/ai-and-its-im...         NaN   \n",
       "3       4  https://insights.blackcoffer.com/how-do-deep-l...         NaN   \n",
       "4       5  https://insights.blackcoffer.com/how-artificia...         NaN   \n",
       "\n",
       "   Unnamed: 3  Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  \\\n",
       "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "   Unnamed: 9  ...  Unnamed: 16  Unnamed: 17  Unnamed: 18  Unnamed: 19  \\\n",
       "0         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "1         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "2         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "3         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "4         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 20  Unnamed: 21  Unnamed: 22  Unnamed: 23  Unnamed: 24  \\\n",
       "0          NaN          NaN          NaN          NaN          NaN   \n",
       "1          NaN          NaN          NaN          NaN          NaN   \n",
       "2          NaN          NaN          NaN          NaN          NaN   \n",
       "3          NaN          NaN          NaN          NaN          NaN   \n",
       "4          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 25  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('input.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "      <th>Unnamed: 24</th>\n",
       "      <th>Unnamed: 25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  Unnamed: 2  \\\n",
       "0       1  https://insights.blackcoffer.com/how-is-login-...         NaN   \n",
       "1       2  https://insights.blackcoffer.com/how-does-ai-h...         NaN   \n",
       "2       3  https://insights.blackcoffer.com/ai-and-its-im...         NaN   \n",
       "3       4  https://insights.blackcoffer.com/how-do-deep-l...         NaN   \n",
       "4       5  https://insights.blackcoffer.com/how-artificia...         NaN   \n",
       "\n",
       "   Unnamed: 3  Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  \\\n",
       "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "   Unnamed: 9  ...  Unnamed: 16  Unnamed: 17  Unnamed: 18  Unnamed: 19  \\\n",
       "0         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "1         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "2         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "3         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "4         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 20  Unnamed: 21  Unnamed: 22  Unnamed: 23  Unnamed: 24  \\\n",
       "0          NaN          NaN          NaN          NaN          NaN   \n",
       "1          NaN          NaN          NaN          NaN          NaN   \n",
       "2          NaN          NaN          NaN          NaN          NaN   \n",
       "3          NaN          NaN          NaN          NaN          NaN   \n",
       "4          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 25  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "links = [x for x in df['URL']]\n",
    "# print(links)\n",
    "print(type(links))\n",
    "print(type(df['URL']))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 170 reports saved\n"
     ]
    }
   ],
   "source": [
    "# session = requests.Session()\n",
    "# session.headers = {\n",
    "#     # \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.1.2222.33 Safari/537.36\",\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (X11; U; Linux i686; en-US) AppleWebKit/532.9 (KHTML, like Gecko) Chrome/5.0.307.11 Safari/532.9\",\n",
    "#     \"Accept-Encoding\": \"*\",\n",
    "#     \"Connection\": \"keep-alive\"\n",
    "# }\n",
    "# reports = []\n",
    "# for url in links:\n",
    "#     r = session.get(url)#requests.get(url)\n",
    "#     data = r.text\n",
    "#     soup = BeautifulSoup(data, \"html.parser\")\n",
    "#     reports.append(soup.get_text())\n",
    "    \n",
    "# print(f'Total {len(reports)} reports saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# print(type(reports))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Sentimental Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of Stop Words are 121\n",
      "['ABOUT', 'ABOVE', 'AFTER', 'AGAIN', 'ALL', 'AM', 'AMONG', 'AN', 'AND', 'ANY', 'ARE', 'AS', 'AT', 'BE', 'BECAUSE', 'BEEN', 'BEFORE', 'BEING', 'BELOW', 'BETWEEN', 'BOTH', 'BUT', 'BY', 'CAN', 'DID', 'DO', 'DOES', 'DOING', 'DOWN', 'DURING', 'EACH', 'FEW', 'FOR', 'FROM', 'FURTHER', 'HAD', 'HAS', 'HAVE', 'HAVING', 'HE', 'HER', 'HERE', 'HERS', 'HERSELF', 'HIM', 'HIMSELF', 'HIS', 'HOW', 'IF', 'IN', 'INTO', 'IS', 'IT', 'ITS', 'ITSELF', 'JUST', 'ME', 'MORE', 'MOST', 'MY', 'MYSELF', 'NO', 'NOR', 'NOT', 'NOW', 'OF', 'OFF', 'ON', 'ONCE', 'ONLY', 'OR', 'OTHER', 'OUR', 'OURS', 'OURSELVES', 'OUT', 'OVER', 'OWN', 'SAME', 'SHE', 'SHOULD', 'SO', 'SOME', 'SUCH', 'THAN', 'THAT', 'THE', 'THEIR', 'THEIRS', 'THEM', 'THEMSELVES', 'THEN', 'THERE', 'THESE', 'THEY', 'THIS', 'THOSE', 'THROUGH', 'TO', 'TOO', 'UNDER', 'UNTIL', 'UP', 'VERY', 'WAS', 'WE', 'WERE', 'WHAT', 'WHEN', 'WHERE', 'WHICH', 'WHILE', 'WHO', 'WHOM', 'WHY', 'WITH', 'YOU', 'YOUR', 'YOURS', 'YOURSELF', 'YOURSELVES']\n"
     ]
    }
   ],
   "source": [
    "with open('StopWords_Generic.txt','r') as f:\n",
    "    stop_words = f.read()\n",
    "\n",
    "stop_words = stop_words.split('\\n')\n",
    "print(f'Total count of Stop Words are {len(stop_words)}')\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Seq_num</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word Proportion</th>\n",
       "      <th>Average Proportion</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Strong_Modal</th>\n",
       "      <th>Weak_Modal</th>\n",
       "      <th>Constraining</th>\n",
       "      <th>Complexity</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARDVARK</td>\n",
       "      <td>1</td>\n",
       "      <td>312</td>\n",
       "      <td>1.422050e-08</td>\n",
       "      <td>1.335201e-08</td>\n",
       "      <td>3.700747e-06</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AARDVARKS</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.367356e-10</td>\n",
       "      <td>8.882163e-12</td>\n",
       "      <td>9.362849e-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABACI</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4.102067e-10</td>\n",
       "      <td>1.200533e-10</td>\n",
       "      <td>5.359747e-08</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABACK</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>6.836779e-10</td>\n",
       "      <td>4.080549e-10</td>\n",
       "      <td>1.406914e-07</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABACUS</td>\n",
       "      <td>5</td>\n",
       "      <td>8009</td>\n",
       "      <td>3.650384e-07</td>\n",
       "      <td>3.798698e-07</td>\n",
       "      <td>3.523914e-05</td>\n",
       "      <td>1058</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Seq_num  Word Count  Word Proportion  Average Proportion  \\\n",
       "0   AARDVARK        1         312     1.422050e-08        1.335201e-08   \n",
       "1  AARDVARKS        2           3     1.367356e-10        8.882163e-12   \n",
       "2      ABACI        3           9     4.102067e-10        1.200533e-10   \n",
       "3      ABACK        4          15     6.836779e-10        4.080549e-10   \n",
       "4     ABACUS        5        8009     3.650384e-07        3.798698e-07   \n",
       "\n",
       "        Std Dev  Doc Count  Negative  Positive  Uncertainty  Litigious  \\\n",
       "0  3.700747e-06         96         0         0            0          0   \n",
       "1  9.362849e-09          1         0         0            0          0   \n",
       "2  5.359747e-08          7         0         0            0          0   \n",
       "3  1.406914e-07         14         0         0            0          0   \n",
       "4  3.523914e-05       1058         0         0            0          0   \n",
       "\n",
       "   Strong_Modal  Weak_Modal  Constraining  Complexity  Syllables     Source  \n",
       "0             0           0             0           0          2  12of12inf  \n",
       "1             0           0             0           0          2  12of12inf  \n",
       "2             0           0             0           0          3  12of12inf  \n",
       "3             0           0             0           0          2  12of12inf  \n",
       "4             0           0             0           0          3  12of12inf  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_dic = pd.read_excel('LoughranMcDonald_MasterDictionary_2020.xlsx')\n",
    "master_dic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainity = pd.read_excel('uncertainty_dictionary.xlsx')\n",
    "uncertainity_words = list(uncertainity['Word'])\n",
    "\n",
    "constraining = pd.read_excel('constraining_dictionary.xlsx')\n",
    "constraining_words = list(constraining['Word'])\n",
    "# print(uncertainity_words)\n",
    "# print(constraining_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentiment(score):\n",
    "    if(score < -0.5):\n",
    "        return 'Most Negative'\n",
    "    elif(score >= -0.5 and score < 0):\n",
    "        return 'Negative'\n",
    "    elif(score == 0):\n",
    "        return 'Neutral'\n",
    "    elif(score > 0 and score < 0.5):\n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return 'Very Positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1  Cleaning using Stop Words Lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words, stop_words):\n",
    "    return [x for x in words if x not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=remove_stopwords(['ABOVE', 'AFTER','AGAIN', 'why', 'are','u','sad'], ['i', 'e','o','u'])\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Creating dictionary of Positive and Negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total positve words in dictionary are 354\n",
      "Total negative words in dictionary are 2355\n"
     ]
    }
   ],
   "source": [
    "positive_dictionary = [x for x in master_dic[master_dic['Positive'] != 0]['Word']]\n",
    "\n",
    "negative_dictionary = [x for x in master_dic[master_dic['Negative'] != 0]['Word']]\n",
    "\n",
    "print(f\"Total positve words in dictionary are {len(positive_dictionary)}\")\n",
    "print(f\"Total negative words in dictionary are {len(negative_dictionary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Extracting Derived variables\n",
    "* We convert the text into a list of tokens using the nltk tokenize module and use these tokens to calculate the 4 variables described below:\n",
    "* Positive Score: This score is calculated by assigning the value of +1 for each word if found in the Positive Dictionary and then adding up all the values.\n",
    "* Negative Score: This score is calculated by assigning the value of -1 for each word if found in the Negative Dictionary and then adding up all the values. We multiply the score with -1 so that the score is a positive number.\n",
    "- Polarity Score: This is the score that determines if a given text is positive or negative in nature. It is calculated by using the formula: \n",
    "~ Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "Range is from -1 to +1\n",
    "* Subjectivity Score: This is the score that determines if a given text is objective or subjective. It is calculated by using the formula: \n",
    "~ Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
    "Range is from 0 to +1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def countfunc(store, words):\n",
    "    score = 0\n",
    "    for x in words:\n",
    "        if(x in store):\n",
    "            score = score+1\n",
    "    return score\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(r'[^A-Za-z]',' ',text.upper())\n",
    "    tokenized_words = word_tokenize(text)\n",
    "    return tokenized_words\n",
    "\n",
    "def positive_score(positive_dictionary,words):\n",
    "    return countfunc(positive_dictionary, words)\n",
    "\n",
    "def negative_score(negative_dictionary, words):\n",
    "    return countfunc(negative_dictionary, words)\n",
    "\n",
    "\n",
    "def polarity(positive_score, negative_score):\n",
    "    return (positive_score - negative_score)/((positive_score + negative_score)+ 0.000001)\n",
    "     \n",
    "# SUBJECTIVITY_SCORE = subjectivity(POLARITY_SCORE, NEGATIVE_SCORE, WORD_COUNT)\n",
    "def subjectivity(positive_score, negative_score, WORD_COUNT):\n",
    "    return (positive_score+negative_score)/(WORD_COUNT+ 0.000001)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(countfunc(['what', 'what','is','is','is'],['is','what','is','asdsa','asdsad','hi']))\n",
    "# POSITIVE_SCORE = countfunc(positive_dictionary, words)\n",
    "# print(POSITIVE_SCORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Analysis of Readability\n",
    "* Analysis of Readability is calculated using the Gunning Fox index formula described below.\n",
    "* Average Sentence Length = the number of words / the number of sentences\n",
    "* Percentage of Complex words = the number of complex words / the number of words \n",
    "* Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fog_index_cal(average_sentence_length, percentage_complexwords):\n",
    "    return 0.4*(average_sentence_length + percentage_complexwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Average Number of Words Per Sentence\n",
    "\n",
    "* Average Number of Words Per Sentence = the total number of words / the total number of sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_number_of_words_per_sentence(total_words,totalsentances):\n",
    "    return total_words/totalsentances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Complex Word Count\n",
    "* Complex words are words in the text that contain more than two syllables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_morethan2(word):\n",
    "    if(len(word) < 2 and (word[-2:] == 'es' or word[-2:] == 'ed')):\n",
    "        return False\n",
    "    \n",
    "    count =0\n",
    "    vowels = ['a','e','i','o','u']\n",
    "    for i in word:\n",
    "        if(i.lower() in vowels):\n",
    "            count = count +1\n",
    "    # print(count)\n",
    "    if(count > 2):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Word Count\n",
    "\n",
    "* We count the total cleaned words present in the text by \n",
    "1.\tremoving the stop words (using stopwords class of nltk package).\n",
    "2.\tremoving any punctuations like ? ! , . from the word before counting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def word_count(content):\n",
    "    tokenized_words = tokenize(content)\n",
    "    #print(f'Total tokenized words are {len(tokenized_words)}')\n",
    "    words = remove_stopwords(tokenized_words, stop_words)\n",
    "    WORD_COUNT = len(words)\n",
    "    return WORD_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WHY', 'R', 'U', 'SAD', 'BRO']\n"
     ]
    }
   ],
   "source": [
    "# print(tokenize('why r u sad bro??'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Syllable Count Per Word\n",
    "\n",
    "* We count the number of Syllables in each word of the text by counting the vowels present in each word. We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def syllable_count_per_word(words):\n",
    "    syllables_count =0\n",
    "    vowels = ['a','e','i','o','u']\n",
    "    for word in words:\n",
    "        for i in word:\n",
    "            if(i.lower() in vowels):\n",
    "                syllables_count = syllables_count +1\n",
    "\n",
    "        if(word[-2:] == 'es' or word[-2:] == 'ed'):\n",
    "            syllables_count = syllables_count - 1\n",
    "            \n",
    "    return syllables_count/len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(syllable_count_per_word([\"alok\", 'pateled','eed','e','ed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def syllable_count(word):\n",
    "    count =0\n",
    "    vowels = ['a','e','i','o','u']\n",
    "    for i in word:\n",
    "        if(i.lower() in vowels):\n",
    "            count = count +1\n",
    "\n",
    "    if(word[-2:] == 'es' or word[-2:] == 'ed'):\n",
    "        count = count - 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# print(syllable_count(\"alok jdaled lkdsjfldes pateled\"))\n",
    "# print(syllable_count(\"al\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "1.2\n"
     ]
    }
   ],
   "source": [
    "# SYLLABLES_COUNT = 0\n",
    "# SYLLABLE_PER_WORD = 0\n",
    "# words = [\"alok\", 'pateled','eed','e','ed']\n",
    "# for word in words:\n",
    "#     SYLLABLES_COUNT = SYLLABLES_COUNT + syllable_count(word)\n",
    "    \n",
    "# SYLLABLE_PER_WORD = SYLLABLES_COUNT/len(words)\n",
    "# print(SYLLABLES_COUNT)                \n",
    "# print(SYLLABLE_PER_WORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.Personal Pronouns\n",
    "* To calculate Personal Pronouns mentioned in the text, we use regex to find the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken so that the country name US is not included in the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sre_yield in c:\\programdata\\anaconda3\\lib\\site-packages (1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ytz (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ytz (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ytz (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ytz (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install sre_yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sre_yield\n",
    "\n",
    "def count_personal_pronoun(content):\n",
    "    all_permuted_pronoun = []\n",
    "    a=[]\n",
    "    pattern=r'(W|w)(e|E) | (I|i) | (O|o)(U|u)(r|R)(s|S) | (M|m)(Y|y) | (Us)|(uS)|(us)'\n",
    "    for each in sre_yield.AllStrings(pattern):\n",
    "        # if each != \"US\":\n",
    "            all_permuted_pronoun.append(each)\n",
    "    # all_permuted_pronoun.remove('US')\n",
    "    # print(all_permuted_pronoun)\n",
    "    return countfunc(content, all_permuted_pronoun)\n",
    "\n",
    "\n",
    "# def count_personal_pronouns(words):\n",
    "#     count = 0\n",
    "#     for word in words:\n",
    "#         pattern = re.compile(r'')\n",
    "#         matches = pattern.findall(word)\n",
    "#         count = count + matches\n",
    "#     return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=\"How US US US is Login Logout Time Tracking for Employees in Office done by AI? - Blackcoffer Insights When people hear AI they often think about sentient robots and magic boxes. AI today is much more mundane and simple—but that doesn’t mean it’s not powerful. Another misconception is that high-profile research projects can be applied directly to any business situation. AI done right can create an extreme return on investments (ROIs)—for instance through automation or precise prediction. But it does take thought, time, and proper implementation. We have seen that success and value generated by AI projects are increased when there is a grounded understanding and expectation of what the technology can deliver from the C-suite down.“Artificial Intelligence (AI) is a science and a set of computational technologies that are inspired by—but typically operate quite differently from—the ways people use their nervous systems and bodies to sense, learn, reason and take action.”3 Lately there has been a big rise in the day-to-day use of machines powered by AI. These machines are wired using cross-disciplinary approaches based on mathematics, computer science, statistics, psychology, and more.4 Virtual assistants are becoming more common, most of the web shops predict your purchases, many companies make use of chatbots in their customer service and many companies use algorithms to detect fraud.AI and Deep Learning technology employed in office entry systems will bring proper time tracking of each employee. As this system tries to learn each person with an image processing technology whose data is feed forwarded to a deep learning model where Deep learning isn’t an algorithm per se, but rather a family of algorithms that implements deep networks (many layers). These networks are so deep that new methods of computation, such as graphics processing units (GPUs), are required to train them, in addition to clusters of compute nodes. So using deep learning we can take detect the employee using face and person recognition scan and through which login, logout timing is recorded. Using an AI system we can even identify each employee’s entry time, their working hours, non-working hours by tracking the movement of an employee in the office so that system can predict and report HR for the salary for each employee based on their working hours. Our system can take feed from CCTV to track movements of employees and this system is capable of recognizing a person even he/she is being masked as in this pandemic situation by taking their iris scan. With this system installed inside the office, the following are some of the benefits:1)Compliance/litigation neeFor several countries, regulations insist that the employer must keep documents available that can demonstrate the working hours performed by each employee. In the event of control from the labor inspectorate or a dispute with an employee, the employer must be able to explain and justify the working hours for the company. This can be made easy as our system is tracking employee movements2)Information security needsThis is about monitoring user connection times to detect suspicious access times. In the event where compromised credentials are used to log on at 3 a.m. on a Saturday, a notification on this access could alert the IT team that an attack is possibly underway3)Employee login logout softwareTo manage and react to employees’ attendance, overtime thresholds, productivity, and suspicious access times, our system records and stores detailed and interactive reporting on users’ connection times. These records allow you to better manage users’ connection times and provide accurate, detailed data required by management.4)If you want to avoid paying overtime, make sure that your employees respect certain working time quotas or even avoid suspicious access. Our system will alert the HR officer about each employee’s office in and out time so that they can accordingly take action.5)Last but not least it reduces human resource needs to keep track of the records and sending the report to HR and HR officials has to check through the report so this system will reduce times and human resource needWith the use of AI and Deep Learning technologies, we can automate some routines stuff with more functionality which humans need more resources to keep track thereby reducing time spent on manual data entry works rather companies can think of making their position high in the competitive world.Blackcoffer Insights 33: Suriya E, Vellore Institute of Technology\"\n",
    "count_personal_pronoun(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.Average Word Length\n",
    "* Average Word Length is calculated by the formula=\n",
    "* Sum of the total number of characters in each word / Total number of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words=['alok', 'patel']\n",
    "def average_word_length(words):\n",
    "    all_words_length =0\n",
    "    for word in words:\n",
    "        all_words_length = all_words_length +len(word)\n",
    "    return all_words_length/len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(average_word_length(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13',\n",
       "       'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17',\n",
       "       'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 2', 'Unnamed: 20',\n",
       "       'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24',\n",
       "       'Unnamed: 25', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6',\n",
       "       'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "df2=df[df.columns.difference(['URL_ID', 'URL'])]\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['URL_ID', 'URL'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df=df.drop(df.loc[ : ,df.columns.difference(['URL_ID', 'URL'])], axis=1)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files():\n",
    "    n = 0\n",
    "    while True:\n",
    "        yield open('./allhtml/%d.part' % n, 'w',encoding=\"utf-8\")\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = files()\n",
    "outfile = next(fs)\n",
    "allhtml = []\n",
    "articles = []\n",
    "for url in df['URL']:\n",
    "    session = requests.Session()\n",
    "    session.headers = {\n",
    "        # \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.1.2222.33 Safari/537.36\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; U; Linux i686; en-US) AppleWebKit/532.9 (KHTML, like Gecko) Chrome/5.0.307.11 Safari/532.9\",\n",
    "        \"Accept-Encoding\": \"*\",\n",
    "        \"Connection\": \"keep-alive\"\n",
    "    }\n",
    "    r = session.get(url)\n",
    "    data = r.text \n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    allhtml.append(soup)\n",
    "    # encodedsoup = soup.encode(\"utf-8\")\n",
    "    Title = soup.title.get_text()\n",
    "    Article = soup.find(\"div\", class_= \"td-post-content\").text\n",
    "    strr = str(Title) + str(Article)\n",
    "    articles.append(strr)\n",
    "    outfile = next(fs)\n",
    "    outfile.write(str(Title) + str(Article))\n",
    "    outfile.close()\n",
    "\n",
    "    textfile = open(\"allhtml.txt\", \"w\",encoding=\"utf-8\")\n",
    "    for html in allhtml:\n",
    "        textfile.write(str(html) + \"\\n\"+\"-\"*125+\"\\n\" )\n",
    "    textfile.close()\n",
    "\n",
    "    articlefile = open(\"articles.txt\", \"w\",encoding=\"utf-8\")\n",
    "    for article in articles:\n",
    "        articlefile.write(str(article) + \"\\n\"+\"-\"*125+\"\\n\" )\n",
    "    articlefile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = [\n",
    "      'POSITIVE_SCORE',\n",
    "      'NEGATIVE_SCORE',\n",
    "      'POLARITY_SCORE',\n",
    "      'SUBJECTIVITY_SCORE',\n",
    "      'AVG_SENTENCE_LENGTH',\n",
    "      'PERCENTAGE_OF_COMPLEX_WORDS',\n",
    "      'FOG_INDEX',\n",
    "      'AVG_NUMBER_OF_WORDS_PER_SENTENCE',#\n",
    "      'COMPLEX_WORD_COUNT',\n",
    "      'WORD_COUNT',\n",
    "      'SYLLABLE_PER_WORD',#\n",
    "      'PERSONAL_PRONOUNS',#\n",
    "      'AVG_WORD_LENGTH'#\n",
    "      ]\n",
    "\n",
    "for v in var[:-1]:\n",
    "      df[v] = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE_SCORE</th>\n",
       "      <th>NEGATIVE_SCORE</th>\n",
       "      <th>POLARITY_SCORE</th>\n",
       "      <th>SUBJECTIVITY_SCORE</th>\n",
       "      <th>AVG_SENTENCE_LENGTH</th>\n",
       "      <th>PERCENTAGE_OF_COMPLEX_WORDS</th>\n",
       "      <th>FOG_INDEX</th>\n",
       "      <th>AVG_NUMBER_OF_WORDS_PER_SENTENCE</th>\n",
       "      <th>COMPLEX_WORD_COUNT</th>\n",
       "      <th>WORD_COUNT</th>\n",
       "      <th>SYLLABLE_PER_WORD</th>\n",
       "      <th>PERSONAL_PRONOUNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  POSITIVE_SCORE  \\\n",
       "0       1  https://insights.blackcoffer.com/how-is-login-...             0.0   \n",
       "1       2  https://insights.blackcoffer.com/how-does-ai-h...             0.0   \n",
       "2       3  https://insights.blackcoffer.com/ai-and-its-im...             0.0   \n",
       "3       4  https://insights.blackcoffer.com/how-do-deep-l...             0.0   \n",
       "4       5  https://insights.blackcoffer.com/how-artificia...             0.0   \n",
       "\n",
       "   NEGATIVE_SCORE  POLARITY_SCORE  SUBJECTIVITY_SCORE  AVG_SENTENCE_LENGTH  \\\n",
       "0             0.0             0.0                 0.0                  0.0   \n",
       "1             0.0             0.0                 0.0                  0.0   \n",
       "2             0.0             0.0                 0.0                  0.0   \n",
       "3             0.0             0.0                 0.0                  0.0   \n",
       "4             0.0             0.0                 0.0                  0.0   \n",
       "\n",
       "   PERCENTAGE_OF_COMPLEX_WORDS  FOG_INDEX  AVG_NUMBER_OF_WORDS_PER_SENTENCE  \\\n",
       "0                          0.0        0.0                               0.0   \n",
       "1                          0.0        0.0                               0.0   \n",
       "2                          0.0        0.0                               0.0   \n",
       "3                          0.0        0.0                               0.0   \n",
       "4                          0.0        0.0                               0.0   \n",
       "\n",
       "   COMPLEX_WORD_COUNT  WORD_COUNT  SYLLABLE_PER_WORD  PERSONAL_PRONOUNS  \n",
       "0                 0.0         0.0                0.0                0.0  \n",
       "1                 0.0         0.0                0.0                0.0  \n",
       "2                 0.0         0.0                0.0                0.0  \n",
       "3                 0.0         0.0                0.0                0.0  \n",
       "4                 0.0         0.0                0.0                0.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(articles)):\n",
    "    if articles[i]:\n",
    "            tokenized_words = tokenize(articles[i])\n",
    "            words = remove_stopwords(tokenized_words, stop_words)\n",
    "            WORD_COUNT = len(words)\n",
    "            sentences = sent_tokenize(articles[i])\n",
    "            SENTANCE_COUNT = len(sentences)\n",
    "            COMPLEX_WORD_COUNT =0\n",
    "            SYLLABLE_PER_WORD = 0\n",
    "            for word in words:\n",
    "                if(syllable_morethan2(word)):\n",
    "                    COMPLEX_WORD_COUNT = COMPLEX_WORD_COUNT + 1\n",
    "            \n",
    "            \n",
    "            df.at[i , 'POSITIVE_SCORE']                 = countfunc(positive_dictionary, words)\n",
    "            df.at[i , 'NEGATIVE_SCORE']                 = countfunc(negative_dictionary, words)\n",
    "            df.at[i , 'POLARITY_SCORE']                 = polarity(POSITIVE_SCORE, NEGATIVE_SCORE)\n",
    "            df.at[i , 'SUBJECTIVITY_SCORE']             =subjectivity(POLARITY_SCORE, NEGATIVE_SCORE, WORD_COUNT)\n",
    "            df.at[i , 'AVG_SENTENCE_LENGTH']            =WORD_COUNT/SENTANCE_COUNT\n",
    "            df.at[i , 'PERCENTAGE_OF_COMPLEX_WORDS']    =COMPLEX_WORD_COUNT/WORD_COUNT\n",
    "            df.at[i , 'FOG_INDEX']                      =fog_index_cal(AVG_SENTENCE_LENGTH, PERCENTAGE_OF_COMPLEX_WORDS)\n",
    "            df.at[i , 'AVG_NUMBER_OF_WORDS_PER_SENTENCE']=WORD_COUNT/SENTANCE_COUNT\n",
    "            df.at[i , 'COMPLEX_WORD_COUNT']             =COMPLEX_WORD_COUNT\n",
    "            df.at[i , 'WORD_COUNT']                     =WORD_COUNT\n",
    "            df.at[i , 'SYLLABLE_PER_WORD']              =syllable_count_per_word(words)\n",
    "            df.at[i , 'PERSONAL_PRONOUNS']              =count_personal_pronoun(articles[i])\n",
    "            df.at[i , 'AVG_WORD_LENGTH']                =average_word_length(words)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('output.xlsx')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
